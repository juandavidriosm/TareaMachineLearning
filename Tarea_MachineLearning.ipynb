{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7023d01-fd80-4a9a-9512-2243367748a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import jd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder,LabelEncoder,OneHotEncoder,TargetEncoder\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from feature_engine.encoding import OrdinalEncoder as feat_eng_OrdinalCategoricalEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46734982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4294dccb",
   "metadata": {},
   "source": [
    "import pycaret\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c0342-b327-4dd8-9a1c-d9141111d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df.status_group = le.fit_transform(df.status_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in df.dtypes.items():\n",
    "    if j == \"object\":\n",
    "        df[i] = df[i].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          int64\n",
       "amount_tsh                float64\n",
       "date_recorded            category\n",
       "funder                   category\n",
       "gps_height                  int64\n",
       "installer                category\n",
       "longitude                 float64\n",
       "latitude                  float64\n",
       "wpt_name                 category\n",
       "num_private                 int64\n",
       "basin                    category\n",
       "subvillage               category\n",
       "region                   category\n",
       "region_code                 int64\n",
       "district_code               int64\n",
       "lga                      category\n",
       "ward                     category\n",
       "population                  int64\n",
       "public_meeting           category\n",
       "recorded_by              category\n",
       "scheme_management        category\n",
       "scheme_name              category\n",
       "permit                   category\n",
       "construction_year           int64\n",
       "extraction_type          category\n",
       "extraction_type_group    category\n",
       "extraction_type_class    category\n",
       "management               category\n",
       "management_group         category\n",
       "payment                  category\n",
       "payment_type             category\n",
       "water_quality            category\n",
       "quality_group            category\n",
       "quantity                 category\n",
       "quantity_group           category\n",
       "source                   category\n",
       "source_type              category\n",
       "source_class             category\n",
       "waterpoint_type          category\n",
       "waterpoint_type_group    category\n",
       "status_group                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc8276",
   "metadata": {},
   "source": [
    "num_cols = df.select_dtypes(include = np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"status_group\")\n",
    "\n",
    "cat_cols= df.select_dtypes(exclude = np.number).columns.tolist()\n",
    "cat_cols.remove(\"date_recorded\")\n",
    "cat_cols.remove(\"funder\")\n",
    "cat_cols.remove(\"installer\")\n",
    "cat_cols.remove(\"wpt_name\")\n",
    "cat_cols.remove(\"basin\")\n",
    "\n",
    "\n",
    "exp = setup(data=df, target='status_group', \n",
    "            numeric_features=num_cols, \n",
    "           categorical_features=[\"waterpoint_type_group\",\"date_recorded\",\"installer\",\"wpt_name\",\"basin\",\"subvillage\",\"region\",\n",
    "                                 \"lga\",\"ward\",#\"public_meeting\",\"recorded_by\",\"scheme_management\",\"scheme_name\"\n",
    "                                 ],\n",
    "            ignore_features=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394aa4a",
   "metadata": {},
   "source": [
    "best = exp.compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8a8a0",
   "metadata": {},
   "source": [
    "exp.plot_model(best,\"confusion_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06dd126",
   "metadata": {},
   "source": [
    "pred = exp.predict_model(best,X_test)\n",
    "X_test_labels_pycaret = pred[[\"id\",\"prediction_label\"]]\n",
    "X_test_labels_pycaret[\"prediction_label\"] = le.inverse_transform(X_test_labels_pycaret.prediction_label)\n",
    "X_test_labels_pycaret.rename(columns={\"prediction_label\":\"status_group\"},inplace=True)\n",
    "X_test_labels_pycaret.to_csv(\"Out/predicciones_pycaret1.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd0732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50785</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17168</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45559</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49871</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14845</th>\n",
       "      <td>39307</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14846</th>\n",
       "      <td>18990</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14847</th>\n",
       "      <td>28749</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>33492</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14849</th>\n",
       "      <td>68707</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    status_group\n",
       "0      50785      functional\n",
       "1      51630  non functional\n",
       "2      17168  non functional\n",
       "3      45559  non functional\n",
       "4      49871      functional\n",
       "...      ...             ...\n",
       "14845  39307      functional\n",
       "14846  18990      functional\n",
       "14847  28749      functional\n",
       "14848  33492      functional\n",
       "14849  68707  non functional\n",
       "\n",
       "[14850 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Out/predicciones_pycaret1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783baa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f59fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "#df.status_group = le.fit_transform(df.status_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79857121",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "# drop list baja V de Cramer para que no estorben, posteriormente cambio filosofía.\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', TargetEncoder(), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46ca117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y.status_group.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab923aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"predictions\"] = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6e4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.predictions = le.inverse_transform(X_test.predictions)\n",
    "X_test = X_test[[\"remainder__id\",\"predictions\"]]\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "X_test.to_csv(\"Out/predicciones_TargetEncoderStandardScaler.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b98b6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e3e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c9f3dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_enc__date_recorded            0\n",
       "target_enc__funder                   0\n",
       "target_enc__installer                0\n",
       "target_enc__wpt_name                 0\n",
       "target_enc__basin                    0\n",
       "target_enc__subvillage               0\n",
       "target_enc__region                   0\n",
       "target_enc__lga                      0\n",
       "target_enc__ward                     0\n",
       "target_enc__public_meeting           0\n",
       "target_enc__scheme_management        0\n",
       "target_enc__scheme_name              0\n",
       "target_enc__extraction_type          0\n",
       "target_enc__extraction_type_group    0\n",
       "target_enc__extraction_type_class    0\n",
       "target_enc__management               0\n",
       "target_enc__payment                  0\n",
       "target_enc__payment_type             0\n",
       "target_enc__water_quality            0\n",
       "target_enc__quality_group            0\n",
       "target_enc__quantity                 0\n",
       "target_enc__quantity_group           0\n",
       "target_enc__source                   0\n",
       "target_enc__source_type              0\n",
       "target_enc__source_class             0\n",
       "target_enc__waterpoint_type          0\n",
       "target_enc__waterpoint_type_group    0\n",
       "target_enc__district_code            0\n",
       "target_enc__region_code              0\n",
       "scaler__gps_height                   0\n",
       "scaler__longitude                    0\n",
       "scaler__latitude                     0\n",
       "scaler__construction_year            0\n",
       "remainder__id                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f3cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y.status_group.values)\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee452c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/predicciones_OrdinalEncoderStandardScaler.csv\",index=None,sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267ecb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac635fa",
   "metadata": {},
   "source": [
    "# Con Clusters de location, tenía altas espectativas, pero no dio buen resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c84da04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = OrdinalEncoder()\n",
    "label_enc = LabelEncoder()\n",
    "X= pd.read_csv(\"Out/XReemplazoLongitudeLatitudeConClusters.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"Out/X_testReemplazoLongitudeLatitudeConClusters.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "    \n",
    "y[\"status_group\"] = label_enc.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y.status_group.values)\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fac16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = label_enc.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/predicciones_OrdinalEncoderStandardScaler7ClustersLoc.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425e800",
   "metadata": {},
   "source": [
    "# GradientBoosting da buenos resultados en cross_validate, es extraño que el GradientBoosting le gane al random forest en cross validate, pero sea mucho peor en el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb876184",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb4c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac3a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = GradientBoostingClassifier()\n",
    "gradBoost.fit(X.to_numpy(),y.status_group.values)\n",
    "X_test[\"status_group\"] = gradBoost.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6ef1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/GradBoostpredicciones_OrdinalEncoderStandardScaler.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd7b6d2",
   "metadata": {},
   "source": [
    "# Empiezo a probra resampling, todos empeoran el score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f443d",
   "metadata": {},
   "source": [
    "## SMOTE not majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ca558b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "sm = SMOTE(random_state=23,sampling_strategy = \"not majority\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "X, y = sm.fit_resample(X, y.status_group.values)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y)\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d33073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/RandomForestSMOTEnot_majority.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a73c87",
   "metadata": {},
   "source": [
    "## SMOTE minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf01acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "sm = SMOTE(random_state=23,sampling_strategy = \"minority\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "X, y = sm.fit_resample(X, y.status_group.values)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y)\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04c0bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/RandomForestSMOTEminority.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6d1dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queda por intentar con más técnicas, dejando clusters y continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98bd29",
   "metadata": {},
   "source": [
    "## Quiero conocer la distribución de las etiquetas de prueba, para eso utilizaré 3 entregas con una sola etiqueta incluida en ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f70782d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_functional = pd.read_csv(\"Out/RandomForestSMOTEminority.csv\",sep=\",\")\n",
    "solo_functional.status_group = \"functional\"\n",
    "solo_functional.to_csv(\"Out/solo_functional.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ca2252e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non functional', 'functional', 'functional needs repair'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Out/RandomForestSMOTEminority.csv\",sep=\",\").status_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33d198b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_nonfunctional = pd.read_csv(\"Out/RandomForestSMOTEminority.csv\",sep=\",\")\n",
    "solo_nonfunctional.status_group = \"non functional\"\n",
    "solo_nonfunctional.to_csv(\"Out/solo_nonfunctional.csv\",index=None,sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79375c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50785</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17168</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45559</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49871</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14845</th>\n",
       "      <td>39307</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14846</th>\n",
       "      <td>18990</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14847</th>\n",
       "      <td>28749</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>33492</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14849</th>\n",
       "      <td>68707</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id             status_group\n",
       "0      50785  functional needs repair\n",
       "1      51630  functional needs repair\n",
       "2      17168  functional needs repair\n",
       "3      45559  functional needs repair\n",
       "4      49871  functional needs repair\n",
       "...      ...                      ...\n",
       "14845  39307  functional needs repair\n",
       "14846  18990  functional needs repair\n",
       "14847  28749  functional needs repair\n",
       "14848  33492  functional needs repair\n",
       "14849  68707  functional needs repair\n",
       "\n",
       "[14850 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solo_functional_needs_repair = pd.read_csv(\"Out/RandomForestSMOTEminority.csv\",sep=\",\")\n",
    "solo_functional_needs_repair.status_group = \"functional needs repair\"\n",
    "solo_functional_needs_repair.to_csv(\"Out/solo_functional_needs_repair.csv\",index=None,sep=\",\")\n",
    "solo_functional_needs_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e489ed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solo_functional = \"0.5461\"\n",
    "solo_nonfunctional = \"0.3820\"\n",
    "solo_functional_needs_repair = \"0.0719\"\n",
    "0.5461+0.3820+0.0719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d626493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 0.543081\n",
       "non functional             0.384242\n",
       "functional needs repair    0.072677\n",
       "Name: status_group, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"In/Training_set_labels.csv\").status_group.value_counts(normalize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f09025",
   "metadata": {},
   "source": [
    "# Intento distintos encoders de missings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7f87e",
   "metadata": {},
   "source": [
    "#### \"https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be0f2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list  = [\"scheme_name\",\"funder\",\"management_group\",\"num_private\",\"recorded_by\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source\",\"source_class\",\"source_type\",\"waterpoint_type_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb29c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "X.district_code = X.district_code.astype(\"category\")\n",
    "X.region_code = X.region_code.astype(\"category\")\n",
    "\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "for i in X_test.select_dtypes(include = \"O\").columns:\n",
    "    X_test[i] = X[i].astype(\"category\")\n",
    "X_test.district_code = X_test.district_code.astype(\"category\")\n",
    "X_test.region_code = X_test.region_code.astype(\"category\")\n",
    "\n",
    "\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y.status_group)\n",
    "\n",
    "X.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "\n",
    "X_test.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X_test.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering = KMeans(n_clusters=20)\n",
    "#df_geo = df[[\"longitude\",\"latitude\"]]\n",
    "#df_geo[\"location_cluster\"] = clustering.fit_predict(df_geo)\n",
    "\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be667f",
   "metadata": {},
   "source": [
    "### Primero debo ver cómo se comporta el score con los drops llevados a cabo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb3680b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'amount_tsh',\n",
       " 'gps_height',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'population',\n",
       " 'construction_year',\n",
       " 'amount_tsh_binned',\n",
       " 'location_cluster']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1940b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y)\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5fc89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/MismoRFBuenoConMasDropsClustersYCont.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac0b08d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50785</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17168</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45559</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49871</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14845</th>\n",
       "      <td>39307</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14846</th>\n",
       "      <td>18990</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14847</th>\n",
       "      <td>28749</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>33492</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14849</th>\n",
       "      <td>68707</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    status_group\n",
       "0      50785      functional\n",
       "1      51630      functional\n",
       "2      17168      functional\n",
       "3      45559  non functional\n",
       "4      49871      functional\n",
       "...      ...             ...\n",
       "14845  39307  non functional\n",
       "14846  18990      functional\n",
       "14847  28749      functional\n",
       "14848  33492      functional\n",
       "14849  68707      functional\n",
       "\n",
       "[14850 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb909a",
   "metadata": {},
   "source": [
    "# ¿Esto dio un score de 0.5????? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83750249",
   "metadata": {},
   "source": [
    "## Tendré que volver a intentar pero sin drop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6b2dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list =[]\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "X.district_code = X.district_code.astype(\"category\")\n",
    "X.region_code = X.region_code.astype(\"category\")\n",
    "\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "for i in X_test.select_dtypes(include = \"O\").columns:\n",
    "    X_test[i] = X[i].astype(\"category\")\n",
    "X_test.district_code = X_test.district_code.astype(\"category\")\n",
    "X_test.region_code = X_test.region_code.astype(\"category\")\n",
    "\n",
    "\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y.status_group)\n",
    "\n",
    "X.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 0\n",
    "\n",
    "X_test.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X_test.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 0\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering = KMeans(n_clusters=20)\n",
    "#df_geo = df[[\"longitude\",\"latitude\"]]\n",
    "#df_geo[\"location_cluster\"] = clustering.fit_predict(df_geo)\n",
    "\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y)\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/MismoRFSinDrops.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0592fd",
   "metadata": {},
   "source": [
    "# Volvió a dar 0.5, no entiendo lo que ocurrió con este submission, probablemente un error de Jupyter de alguna variable ya creada en el entorno estorbando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88720884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1e4eed6",
   "metadata": {},
   "source": [
    "# Intento ahora mismo código utilizado con SMOTE, pero quitando las dos líneas del resampling a ver si es que la página tiene un problema\n",
    "\n",
    "### 0.8160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e821e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya\n",
      "ya\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =10000000), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "print(\"ya\")\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "print(\"ya\")\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.to_numpy(),y.status_group.values)\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "015a8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/IntentoVolver08.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fecf8",
   "metadata": {},
   "source": [
    "# Esto ya volvió a funcionar y a darme score 0.8, no entiendo bien lo que ocurre. Lo único que pudiese ser es que este código le hace drop a amount_tsh y el resto no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b2755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "005fb9e2",
   "metadata": {},
   "source": [
    "# Menos mal revisé los resultados del cambio a la lista de drop antes de intentar los imputadores, hubiese pensado que el problema venía de la imputación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ca6cd",
   "metadata": {},
   "source": [
    "# Ahora sí intento los imputadores de missings en conjunto a un drop_list sacado con una limpieza de variables que posteriormente se borró por error e intenté reconstruir\n",
    "\n",
    "#### \"https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94b9a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be5195",
   "metadata": {},
   "source": [
    "# KNN imputer, 8170 MEJOR RESULTADO HASTA AHORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39380468",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_imp2 = KNNImputer()\n",
    "\n",
    "# Me imagino que: KNNImputer()==IterativeImputer(estimator=KNeighborsRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65b5507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "X = it_imp2.fit_transform(X)\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X,y.status_group.values)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/KneighborsImputer.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68288e",
   "metadata": {},
   "source": [
    "# random forest imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29c14bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_imp1 = IterativeImputer(RandomForestRegressor(n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "092c3e56",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mfit_transform(X,y\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mstatus_group\u001b[38;5;241m.\u001b[39mvalues), columns\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m     41\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[0;32m---> 43\u001b[0m X \u001b[38;5;241m=\u001b[39m it_imp1\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mya\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#X_test = it_imp1.transform(X)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_iterative.py:766\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_idx \u001b[38;5;129;01min\u001b[39;00m ordered_idx:\n\u001b[1;32m    763\u001b[0m     neighbor_feat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_feat_idx(\n\u001b[1;32m    764\u001b[0m         n_features, feat_idx, abs_corr_mat\n\u001b[1;32m    765\u001b[0m     )\n\u001b[0;32m--> 766\u001b[0m     Xt, estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impute_one_feature(\n\u001b[1;32m    767\u001b[0m         Xt,\n\u001b[1;32m    768\u001b[0m         mask_missing_values,\n\u001b[1;32m    769\u001b[0m         feat_idx,\n\u001b[1;32m    770\u001b[0m         neighbor_feat_idx,\n\u001b[1;32m    771\u001b[0m         estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    772\u001b[0m         fit_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    773\u001b[0m     )\n\u001b[1;32m    774\u001b[0m     estimator_triplet \u001b[38;5;241m=\u001b[39m _ImputerTriplet(\n\u001b[1;32m    775\u001b[0m         feat_idx, neighbor_feat_idx, estimator\n\u001b[1;32m    776\u001b[0m     )\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_\u001b[38;5;241m.\u001b[39mappend(estimator_triplet)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_iterative.py:413\u001b[0m, in \u001b[0;36mIterativeImputer._impute_one_feature\u001b[0;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[1;32m    403\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[1;32m    404\u001b[0m         _safe_indexing(X_filled, neighbor_feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[1;32m    406\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    408\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[1;32m    409\u001b[0m         _safe_indexing(X_filled, feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[1;32m    411\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    412\u001b[0m     )\n\u001b[0;32m--> 413\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# if no missing values, don't predict\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(missing_row_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    493\u001b[0m )(\n\u001b[1;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    495\u001b[0m         t,\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    497\u001b[0m         X,\n\u001b[1;32m    498\u001b[0m         y,\n\u001b[1;32m    499\u001b[0m         sample_weight,\n\u001b[1;32m    500\u001b[0m         i,\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "X = it_imp1.fit_transform(X)\n",
    "print(\"ya\")\n",
    "#X_test = it_imp1.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp1.transform(X_test), columns=it_imp1.get_feature_names_out())\n",
    "print(\"ya\")\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.to_numpy(),y.status_group.values)\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/RFImputer.csv\",index=None,sep=\",\")\n",
    "# 37 minutos y no hace ni el primer print. No vale la pena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c38a7f",
   "metadata": {},
   "source": [
    "# pipeline(Nystroem(),Ridge()) imputer\n",
    "### 0.8153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_imp3 = IterativeImputer(make_pipeline(Nystroem(),Ridge()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efda93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jrios_bipicar\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "X = it_imp3.fit_transform(X)\n",
    "#X_test = it_imp3.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp3.transform(X_test), columns=it_imp3.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X,y.status_group.values)\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/NystroemRidgeImputer.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb2aba",
   "metadata": {},
   "source": [
    "## Vuelvo a intentar cambio en el drop list, en este caso añado los drops de la lista pequeña que no estaban incluidos en la grande, esto lo hago porque la lista grande dió un resultado demasiado parecido a aquel logrado sin ningún drop.\n",
    "\n",
    "#### Dio peor resultado que lista pequeña(0.8170), pero sólo un poco peor.\n",
    "\n",
    "#### 0.8133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "#drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "drop_list  = [\"scheme_name\",\"funder\",\"management_group\",\"num_private\",\"recorded_by\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source\",\"source_class\",\"source_type\",\"waterpoint_type_group\"]\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "X = it_imp2.fit_transform(X)\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X,y.status_group.values)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/KneighborsImputer.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8070b8b",
   "metadata": {},
   "source": [
    "# KNN IMP CON NUEVOS DROPS Y TODOS LOS FEATURES INGENIADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f33add",
   "metadata": {},
   "source": [
    "# 0.850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ae134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juandavid/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "it_imp2 = it_imp2 = KNNImputer()\n",
    "\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "#fechas------------------------------------------------------------------------            \n",
    "\n",
    "X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "\n",
    "\n",
    "\n",
    "X_test.date_recorded = X_test.date_recorded.astype(\"O\")\n",
    "X_test.date_recorded = pd.to_datetime(X_test.date_recorded)\n",
    "X_test['date_recorded'] = (X_test.date_recorded.max() -X_test.date_recorded).dt.days # X.date_recorded.max() a propósito\n",
    "\n",
    "#Droplist-----------------------------------------------------------------------------------------------------------------------------------------------   \n",
    "drop_list  = [\"management_group\",\"num_private\",\"recorded_by\",\"region\",\"region_code\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source_class\",\"source_type\",\"waterpoint_type_group\",\n",
    "              ]#\"permit\",\"population\",\"amount_tsh\" eran drops que funcionaban bien al principio\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "#Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "X.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "\n",
    "X_test.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X_test.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "\n",
    "#Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "from sklearn.cluster import KMeans\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist() \n",
    "cat_cols.append(\"district_code\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "#y_pred = rf.predict(X_test.drop(\"id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/NuevaDropListKNNimpTodasFeatures.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6c74e",
   "metadata": {},
   "source": [
    "## Intento haciendo los drops que hago al principo por baja V de Cramwe con objetivo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cadf99",
   "metadata": {},
   "source": [
    "## 0.8152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309eb65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juandavid/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "it_imp2 = it_imp2 = KNNImputer()\n",
    "\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "#fechas------------------------------------------------------------------------            \n",
    "\n",
    "X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "\n",
    "\n",
    "\n",
    "X_test.date_recorded = X_test.date_recorded.astype(\"O\")\n",
    "X_test.date_recorded = pd.to_datetime(X_test.date_recorded)\n",
    "X_test['date_recorded'] = (X_test.date_recorded.max() -X_test.date_recorded).dt.days # X.date_recorded.max() a propósito\n",
    "\n",
    "#Droplist-----------------------------------------------------------------------------------------------------------------------------------------------   \n",
    "drop_list  = [\"management_group\",\"num_private\",\"recorded_by\",\"region\",\"region_code\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source_class\",\"source_type\",\"waterpoint_type_group\",\n",
    "              \"permit\",\"population\",\"amount_tsh\"] #permit\",\"population\",\"amount_tsh\" eran drops que funcionaban bien al principio\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "\n",
    "#Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "# no se hace porque le hago drop al amount_tsh  \n",
    "\n",
    "#Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "from sklearn.cluster import KMeans\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist() \n",
    "cat_cols.append(\"district_code\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "#y_pred = rf.predict(X_test.drop(\"id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/NuevaDropListMasDropsVCramerKNNimpTodasFeatures.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770c92e",
   "metadata": {},
   "source": [
    "# Sin mis features a ver si están sesgando los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18ea49",
   "metadata": {},
   "source": [
    "## 0.8119 --> algo bueno hacen mis features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_imp2 = KNNImputer()\n",
    "\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "#Droplist-----------------------------------------------------------------------------------------------------------------------------------------------   \n",
    "drop_list  = [\"management_group\",\"num_private\",\"recorded_by\",\"region\",\"region_code\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source_class\",\"source_type\",\"waterpoint_type_group\",\n",
    "              \"permit\",\"population\",\"amount_tsh\"] #permit\",\"population\",\"amount_tsh\" eran drops que funcionaban bien al principio\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist() \n",
    "cat_cols.append(\"district_code\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "#y_pred = rf.predict(X_test.drop(\"id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/NuevaDropListMasDropsVCramerKNNimpNingunFeature.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## deberiái ahora usar la lista que me gusta sacando region code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049cc5f",
   "metadata": {},
   "source": [
    "## knn imputer que da los mejores resultados hasta ahora con mis features\n",
    "\n",
    "## 0.8098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_imp2 = KNNImputer()\n",
    "\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "#fechas------------------------------------------------------------------------            \n",
    "\n",
    "X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "\n",
    "X_test.date_recorded = X_test.date_recorded.astype(\"O\")\n",
    "X_test.date_recorded = pd.to_datetime(X_test.date_recorded)\n",
    "X_test['date_recorded'] = (X_test.date_recorded.max() -X_test.date_recorded).dt.days # X.date_recorded.max() a propósito\n",
    "#-----------------------------------------------\n",
    "\n",
    "drop_list  = [\"scheme_name\",\"funder\",\"management_group\",\"num_private\",\"recorded_by\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source\",\"source_class\",\"source_type\",\"waterpoint_type_group\"]\n",
    "\n",
    "X.drop(drop_list,axis=1,inplace=True)\n",
    "X_test.drop(drop_list,axis=1,inplace=True)\n",
    "#Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "X.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "\n",
    "X_test.loc[X[\"amount_tsh\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "X_test.loc[X[\"amount_tsh\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "\n",
    "#Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "from sklearn.cluster import KMeans\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "#------------------------------------------------------------------------  ------------------------------------------------------------------------  \n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "cat_cols.append(\"district_code\")\n",
    "cat_cols.append(\"region_code\")\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"region_code\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('target_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "X = it_imp2.fit_transform(X)\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X,y.status_group.values)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test)\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/KneighborsImputerYMisFeatures.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e974a4f",
   "metadata": {},
   "source": [
    "## Con función que elige el mejor encoding\n",
    "\n",
    "## drop list con sentido  \n",
    "[\"management_group\",\"num_private\",\"recorded_by\",\"region\",\"region_code\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source_class\",\"source_type\",\"waterpoint_type_group\",\n",
    "              \"permit\",\"population\",\"amount_tsh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e06eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "    drop_list  = [\"management_group\",\"num_private\",\"recorded_by\",\"region\",\"region_code\",\n",
    "              \"extraction_type_group\",\"extraction_type_class\",\"payment\",\"quality_group\",\n",
    "              \"quantity_group\",\"source_class\",\"source_type\",\"waterpoint_type_group\",\n",
    "              \"permit\",\"population\"]\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    clustering = KMeans(n_clusters=20)\n",
    "    X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    \n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eed160",
   "metadata": {},
   "source": [
    "## Con mi función en modo un solo test:\n",
    "### 0.8143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c23359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "it_imp2 = it_imp2 = KNNImputer()\n",
    "\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X= preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "\n",
    "OrdinalEncoder_cols = ['installer', 'basin', 'ward', 'quantity', 'source', 'amount_tsh_binned']\n",
    "\n",
    "OnehotEnc_cols = ['wpt_name', 'subvillage', 'public_meeting', 'scheme_name', 'extraction_type', 'management']\n",
    "\n",
    "TargetEnc_cols = ['water_quality']\n",
    "FrequencyEnc_cols =['funder', 'district_code', 'lga', 'scheme_management', 'payment_type', 'waterpoint_type']\n",
    "\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist() \n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                    unknown_value=10000000,\n",
    "                                    encoded_missing_value =10000001,\n",
    "                                    min_frequency = 0.01), OrdinalEncoder_cols),\n",
    "        #arreglar:\n",
    "        ('onehot', OneHotEncoder(max_categories=7,# Si existen más de 6 valores únicos crea categoría \"infrequent\"\n",
    "                                    handle_unknown =\"infrequent_if_exist\",\n",
    "                                    drop=\"first\",sparse_output=False), OnehotEnc_cols),\n",
    "\n",
    "        ('target', TargetEncoder(target_type=\"multiclass\",\n",
    "                                    cv=2,), TargetEnc_cols),  # default cv 5 me tardaría una eternidad\n",
    "                                    \n",
    "        ('Frequency', CountEncoder(handle_unknown=\"value\",\n",
    "                                    handle_missing=\"value\"), FrequencyEnc_cols),                             \n",
    "        ('scaler', StandardScaler(), num_cols),  \n",
    "    ],\n",
    "    remainder = \"passthrough\",\n",
    "    )\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "#y_pred = rf.predict(X_test.drop(\"id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/MiFuncionModoUnTest.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1320a",
   "metadata": {},
   "source": [
    "## Mi Función en modo KFold\n",
    "### 0.8135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97da95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "it_imp2 = it_imp2 = KNNImputer()\n",
    "\n",
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X= preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "\n",
    "OrdinalEncoder_cols = ['funder', 'quantity', 'source', 'waterpoint_type']\n",
    "\n",
    "OnehotEnc_cols = ['basin', 'district_code', 'ward', 'scheme_management', 'scheme_name', 'extraction_type',\n",
    "                'management', 'payment_type', 'water_quality', 'amount_tsh_binned']\n",
    "\n",
    "TargetEnc_cols = []\n",
    "FrequencyEnc_cols =['installer', 'wpt_name', 'subvillage', 'lga', 'public_meeting']\n",
    "\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist() \n",
    "num_cols.remove(\"district_code\")\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                    unknown_value=10000000,\n",
    "                                    encoded_missing_value =10000001,\n",
    "                                    min_frequency = 0.01), OrdinalEncoder_cols),\n",
    "        #arreglar:\n",
    "        ('onehot', OneHotEncoder(max_categories=7,# Si existen más de 6 valores únicos crea categoría \"infrequent\"\n",
    "                                    handle_unknown =\"infrequent_if_exist\",\n",
    "                                    drop=\"first\",sparse_output=False), OnehotEnc_cols),\n",
    "\n",
    "        ('target', TargetEncoder(target_type=\"multiclass\",\n",
    "                                    cv=2,), TargetEnc_cols),  # default cv 5 me tardaría una eternidad\n",
    "                                    \n",
    "        ('Frequency', CountEncoder(handle_unknown=\"value\",\n",
    "                                    handle_missing=\"value\"), FrequencyEnc_cols),                             \n",
    "        ('scaler', StandardScaler(), num_cols),  \n",
    "    ],\n",
    "    remainder = \"passthrough\",\n",
    "    )\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "#y_pred = rf.predict(X_test.drop(\"id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/MiFuncionModoFold.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223195b",
   "metadata": {},
   "source": [
    "## El rf + knnImputer + la drop_list original pero ahora con todas las features y arreglos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3ab39",
   "metadata": {},
   "source": [
    "# EUREKA!!! 0.8174!! jajajaja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "   \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\",\"construction_year\"]\n",
    "\n",
    "    #el drop list 0.8170 más \"construction_year\"\n",
    "\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    columnas_NA = [\"funder\",\"installer\",\"subvillage\",\"public_meeting\",\"scheme_management\",\"scheme_name\"]#permit\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/KneighborsImputerYMuchoPreproceso.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e75bd6",
   "metadata": {},
   "source": [
    "# Y si en vez de hacerle drop a construction year lo convierto en binaria: 0, no 0\n",
    "\n",
    "## 0.8162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c692095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Construction year------------------------------------------------------------------------  \n",
    "    X[\"construction_year_iscero\"] = X[\"construction_year\"].copy()\n",
    "    X.loc[X[\"construction_year\"]==0,\"construction_year_iscero\"] = 1\n",
    "    X.loc[X[\"construction_year\"]!=0,\"construction_year_iscero\"] = 0\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "   \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\",\"construction_year\"]\n",
    "\n",
    "    #el drop list 0.8170 más \"construction_year\"\n",
    "\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    columnas_NA = [\"funder\",\"installer\",\"subvillage\",\"public_meeting\",\"scheme_management\",\"scheme_name\"]#permit\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72103f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/KneighborsImputerYMuchoPreprocesoMasConstructionYearBinario.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71892c6",
   "metadata": {},
   "source": [
    "# dejando construction year cero como missing. Próxima vez incluir columna en lista de hacer columnas de NA\n",
    "\n",
    "## 0.8176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df403b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "   \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "\n",
    "    #el drop list 0.8170 más \"construction_year\"\n",
    "\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    columnas_NA = [\"funder\",\"installer\",\"subvillage\",\"public_meeting\",\"scheme_management\",\"scheme_name\"]#permit\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05eb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/KneighborsImputerYMuchoPreprocesoMasConstructionYearCeroNAN.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab621ff8",
   "metadata": {},
   "source": [
    "# El de arriba, pero utilizando el encoder que mi función me diga\n",
    "\n",
    "#### 0.7071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784943ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "   \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "\n",
    "    #el drop list 0.8170 más \"construction_year\"\n",
    "\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    columnas_NA = [\"funder\",\"installer\",\"subvillage\",\"public_meeting\",\"scheme_management\",\"scheme_name\",\"construction_year\"]#permit\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "        X[i+\"isNA\"] = X[i+\"isNA\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86797f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "df = pd.merge(X,y,on=\"id\")\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = df[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "OrdinalEncoder_cols = ['region', 'region_code', 'district_code', 'extraction_type_group',\n",
    "                        'extraction_type_class', 'payment', 'quality_group', 'quantity_group',\n",
    "                          'source_type', 'source_class', 'waterpoint_type_group', 'amount_tsh_binned',\n",
    "                            'gps_height_binned', 'population_binned', 'date_recorded_binned', \n",
    "                            'funderisNA', 'installerisNA', 'subvillageisNA', 'public_meetingisNA',\n",
    "                              'scheme_managementisNA', 'scheme_nameisNA', 'construction_yearisNA',\n",
    "                                'public_meeting']\n",
    "\n",
    "OnehotEnc_cols = ['basin', 'waterpoint_type']\n",
    "\n",
    "\n",
    "TargetEnc_cols = ['scheme_management', 'payment_type', 'water_quality']\n",
    "\n",
    "\n",
    "FrequencyEnc_cols = ['installer', 'source']\n",
    "\n",
    "Ordinal_ordered_Encoder_cols = ['funder', 'wpt_name', 'subvillage', 'lga', 'ward', 'scheme_name', 'extraction_type', 'management', 'quantity']\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                    unknown_value=10000000,\n",
    "                                    encoded_missing_value =np.nan,\n",
    "                                    min_frequency = 0.01), OrdinalEncoder_cols),\n",
    "        (\"ordinal_ordered\", feat_eng_OrdinalCategoricalEncoder(encoding_method='ordered',\n",
    "                                                               variables=None,\n",
    "                                                               missing_values='ignore'),Ordinal_ordered_Encoder_cols),\n",
    "        ('onehot', OneHotEncoder(max_categories=7,# Si existen más de 6 valores únicos crea categoría \"infrequent\"\n",
    "                                    handle_unknown =\"infrequent_if_exist\",\n",
    "                                    drop=\"first\",sparse_output=False), OnehotEnc_cols),\n",
    "\n",
    "        ('target', TargetEncoder(target_type=\"multiclass\",\n",
    "                                    cv=2,), TargetEnc_cols),  # default cv 5 me tardaría una eternidad\n",
    "                                    \n",
    "        ('Frequency', CountEncoder(handle_unknown=\"value\",\n",
    "                                    handle_missing=\"return_nan\"), FrequencyEnc_cols),                             \n",
    "        ('scaler', StandardScaler(), num_cols),  \n",
    "    ],\n",
    "    remainder = \"passthrough\",\n",
    "    )\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/RF_KNN_FEATURES_FUNCION.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1156e13",
   "metadata": {},
   "source": [
    "# El mejor, implementación más preprocesado\n",
    "\n",
    "## 0.8178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    X.replace('unknown', np.nan)\n",
    "    X.replace('Unknown', np.nan)\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "   \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "\n",
    "    #el drop list 0.8170 más \"construction_year\"\n",
    "\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    columnas_NA = [\"funder\",\"installer\",\"subvillage\",\"public_meeting\",\"scheme_management\",\"scheme_name\",\"construction_year\"]#permit\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "        X[i+\"isNA\"] = X[i+\"isNA\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248461b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mit_imp2\u001b[38;5;241m.\u001b[39mfit_transform(X), columns\u001b[38;5;241m=\u001b[39mit_imp2\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#X_test = it_imp2.transform(X)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mit_imp2\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mit_imp2\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_knn.py:366\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    358\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    359\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[1;32m    365\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# process_chunk modifies X in place. No return value.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2153\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2152\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2153\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m reduce_func(D_chunk, sl\u001b[38;5;241m.\u001b[39mstart)\n\u001b[1;32m   2154\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_knn.py:349\u001b[0m, in \u001b[0;36mKNNImputer.transform.<locals>.process_chunk\u001b[0;34m(dist_chunk, start)\u001b[0m\n\u001b[1;32m    344\u001b[0m     dist_subset \u001b[38;5;241m=\u001b[39m dist_chunk[dist_idx_map[receivers_idx] \u001b[38;5;241m-\u001b[39m start][\n\u001b[1;32m    345\u001b[0m         :, potential_donors_idx\n\u001b[1;32m    346\u001b[0m     ]\n\u001b[1;32m    348\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neighbors, \u001b[38;5;28mlen\u001b[39m(potential_donors_idx))\n\u001b[0;32m--> 349\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_impute(\n\u001b[1;32m    350\u001b[0m     dist_subset,\n\u001b[1;32m    351\u001b[0m     n_neighbors,\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X[potential_donors_idx, col],\n\u001b[1;32m    353\u001b[0m     mask_fit_X[potential_donors_idx, col],\n\u001b[1;32m    354\u001b[0m )\n\u001b[1;32m    355\u001b[0m X[receivers_idx, col] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_knn.py:184\u001b[0m, in \u001b[0;36mKNNImputer._calc_impute\u001b[0;34m(self, dist_pot_donors, n_neighbors, fit_X_col, mask_fit_X_col)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to impute a single column.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Imputed values for receiver.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Get donors\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m donors_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(dist_pot_donors, n_neighbors \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\n\u001b[1;32m    185\u001b[0m     :, :n_neighbors\n\u001b[1;32m    186\u001b[0m ]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get weight matrix from distance matrix\u001b[39;00m\n\u001b[1;32m    189\u001b[0m donors_dist \u001b[38;5;241m=\u001b[39m dist_pot_donors[\n\u001b[1;32m    190\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(donors_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m], donors_idx\n\u001b[1;32m    191\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:858\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margpartition\u001b[39m\u001b[38;5;124m'\u001b[39m, kth, axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/ReemplazoUnknownPorNaN.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8cd5d",
   "metadata": {},
   "source": [
    "# A ver qué pasa si reemplazo todos los valores \"other - x\" por \"other\"\n",
    "\n",
    "#### 0.8156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54344974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "        #X.basin = X.basin.apply(lambda x: x.split(\" \")[0] if x.split(\" \")[0] == \"Lake\" else x)\n",
    "    for i in ['extraction_type', 'extraction_type_group', 'extraction_type_class',\n",
    "       'management', 'management_group', 'payment', 'payment_type', 'source',\n",
    "       'source_type', 'waterpoint_type', 'waterpoint_type_group']:\n",
    "        \n",
    "        X[i] = X[i].apply(lambda x: 'other' if isinstance(x, str) and x.split(\" \")[0]==\"other\" else x)\n",
    "\n",
    "    X.replace('unknown', np.nan)\n",
    "    X.replace('Unknown', np.nan)\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "   \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]\n",
    "\n",
    "    #el drop list 0.8170 más \"construction_year\"\n",
    "\n",
    "    #Clusters de latitud y longitud------------------------------------------------------------------------   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    columnas_NA = [\"funder\",\"installer\",\"subvillage\",\"public_meeting\",\"scheme_management\",\"scheme_name\",\"construction_year\"]#permit\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "        X[i+\"isNA\"] = X[i+\"isNA\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/ReemplazoOtherXporOOther.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ef305",
   "metadata": {},
   "source": [
    "# Unas cuantas erratas:\n",
    "\n",
    "###  añadirle min freq al ordinal encoder!! \n",
    "### scheme management tiene una a la que mosca con hacerle eso del min freq\n",
    "### X.replace('unknown', np.nan) necesita inplace o asignación\n",
    "### no se actualizó la lista de Nans a la que hacerles columnas propias \n",
    "### además de unknown hay columnas con \"none\"\n",
    "### hacerle lower a unas cuantas columnas\n",
    "### hacer algo con las categorías en train que no salen en test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f273ef1",
   "metadata": {},
   "source": [
    "## 0.8136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_test(X,X_test = None):\n",
    "    \"\"\"\n",
    "    pasa X_test sólo al preprocesar X\n",
    "    \"\"\"\n",
    "        #X.basin = X.basin.apply(lambda x: x.split(\" \")[0] if x.split(\" \")[0] == \"Lake\" else x)\n",
    "    cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\"]\n",
    "\n",
    "    X = X.replace('unknown', np.nan)\n",
    "    X = X.replace('Unknown', np.nan)\n",
    "    X = X.replace('none', np.nan)\n",
    "    cols_pasar_a_lower = [\"installer\",\"scheme_name\",\"wpt_name\"]\n",
    "    for col in cols_pasar_a_lower:\n",
    "        X[col] = X[col].apply(lambda x: str(x).lower())\n",
    "\n",
    "    if X_test is not None:\n",
    "        print(\"si\")\n",
    "        cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\",\n",
    "                                    \"region_code\",\"ward\",\"extraction_type\"]\n",
    "        for i in cols_mas_unique_en_train:\n",
    "            unicos_en_test = X_test[i].unique().tolist()\n",
    "            X[i] = [x if x in unicos_en_test else np.nan for x in X[i].values]\n",
    "            #X[i] = [x if x in unicos_en_test else x.split(\" \")[0] for x in X[i].values]\n",
    "\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #Clusters de latitud y longitud, se hace fuera para que X_test sea transform sin fit------------------------------------------------------------------------\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    \n",
    "    columnas_NA = X.columns[X.isna().any()].tolist()\n",
    "    columnas_NA = [x for x in columnas_NA if x not in ['scheme_name', 'wpt_name', 'ward', 'extraction_type', 'region_code']]# Nas por sobra de categorías, sin nulls en X_test\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "        X[i+\"isNA\"] = X[i+\"isNA\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X_test = preprocess_train_test(X_test)\n",
    "X = preprocess_train_test(X,X_test)\n",
    "\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan,\n",
    "                                  min_frequency = 0.01), cat_cols),  # si menos de 1% de los datos\n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/ArregloErratas.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070b503",
   "metadata": {},
   "source": [
    "## Ahora cambio valores no en X_test a constante \"no_en_test\" en vez de a NAN\n",
    "\n",
    "### 0.8135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57345b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_test(X,X_test = None):\n",
    "    \"\"\"\n",
    "    pasa X_test sólo al preprocesar X\n",
    "    \"\"\"\n",
    "        #X.basin = X.basin.apply(lambda x: x.split(\" \")[0] if x.split(\" \")[0] == \"Lake\" else x)\n",
    "    cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\"]\n",
    "\n",
    "    X = X.replace('unknown', np.nan)# se pasan a nan, pero son muy significativos, por lo que me aseguro de crear columna de que es NAN imputado\n",
    "    X = X.replace('Unknown', np.nan)\n",
    "    X = X.replace('none', np.nan)\n",
    "    \n",
    "    cols_pasar_a_lower = [\"installer\",\"scheme_name\",\"wpt_name\"]\n",
    "    for col in cols_pasar_a_lower:\n",
    "        X[col] = X[col].apply(lambda x: str(x).lower())\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if X_test is not None:\n",
    "        print(\"si\")\n",
    "        cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\",\"ward\",\"extraction_type\"]#\"region_code\"\n",
    "        for i in cols_mas_unique_en_train:\n",
    "            unicos_en_test = X_test[i].unique().tolist()\n",
    "            X[i] = [x if x in unicos_en_test else \"no_en_test\" for x in X[i].values]\n",
    "            #X[i] = [x if x in unicos_en_test else x.split(\" \")[0] for x in X[i].values]\n",
    "        \n",
    "    \n",
    "\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    #X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X[\"funder\"] = X[\"funder\"].replace(\"0\",np.nan)\n",
    "    #X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    #X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    X[\"installer\"] = X[\"installer\"].replace(\"0\",np.nan)\n",
    "    X[\"installer\"] = X[\"installer\"].replace(\"-\",np.nan)\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #Clusters de latitud y longitud, se hace fuera para que X_test sea transform sin fit------------------------------------------------------------------------\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    \n",
    "    columnas_NA = X.columns[X.isna().any()].tolist()\n",
    "    columnas_NA = [x for x in columnas_NA if x not in ['scheme_name', 'wpt_name', 'ward', 'extraction_type', 'region_code']]# Nas por sobra de categorías, sin nulls en X_test\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "        X[i+\"isNA\"] = X[i+\"isNA\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b32f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si\n",
      "preprocesado\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X_test = preprocess_train_test(X_test)\n",
    "X = preprocess_train_test(X,X_test)\n",
    "\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan,\n",
    "                                  min_frequency = 0.01), cat_cols),  # si menos de 1% de los datos\n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/NoEnXtestAConstante.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488abc1d",
   "metadata": {},
   "source": [
    "# QuedandomeConPrimeraParteDeCatsNoEnTest \n",
    "\n",
    "#### 0.8128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f89b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_test(X,X_test = None):\n",
    "    \"\"\"\n",
    "    pasa X_test sólo al preprocesar X\n",
    "    \"\"\"\n",
    "        #X.basin = X.basin.apply(lambda x: x.split(\" \")[0] if x.split(\" \")[0] == \"Lake\" else x)\n",
    "    cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\"]\n",
    "\n",
    "    X = X.replace('unknown', np.nan)# se pasan a nan, pero son muy significativos, por lo que me aseguro de crear columna de que es NAN imputado\n",
    "    X = X.replace('Unknown', np.nan)\n",
    "    X = X.replace('none', np.nan)\n",
    "    \n",
    "    cols_pasar_a_lower = [\"installer\",\"scheme_name\",\"wpt_name\"]\n",
    "    for col in cols_pasar_a_lower:\n",
    "        X[col] = X[col].apply(lambda x: str(x).lower())\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "    if X_test is not None:\n",
    "        print(\"si\")\n",
    "        cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\",\"ward\",\"extraction_type\"]#\"region_code\"\n",
    "        for i in cols_mas_unique_en_train:\n",
    "            unicos_en_test = X_test[i].unique().tolist()\n",
    "            #X[i] = [x if x in unicos_en_test else \"no_en_test\" for x in X[i].values]\n",
    "            X[i] = [x if x in unicos_en_test else str(x).split(\" \")[0] for x in X[i].values]\n",
    "      \n",
    "    \n",
    "\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    X.district_code = X.district_code.astype(\"category\")\n",
    "    X.region_code = X.region_code.astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------   \n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh\"].copy()\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(-np.inf,1.5),\"amount_tsh_binned\"] = 0\n",
    "    X.loc[X[\"amount_tsh_binned\"].between(1.5,np.inf),\"amount_tsh_binned\"] = 1\n",
    "    X[\"amount_tsh_binned\"] = X[\"amount_tsh_binned\"].astype(\"category\")\n",
    "    #gps_heightoptbinning------------------------------------------------------------------------  \n",
    "    X.loc[X[\"gps_height\"]<0,\"gps_height\"] = 0\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height\"].copy()\n",
    "    X.loc[X[\"gps_height\"].between(-np.inf,508),\"gps_height_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"gps_height\"].between(508,850.5),\"gps_height_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"gps_height\"].between(850.5,1359.5),\"gps_height_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"gps_height\"].between(1359.5,1567.50),\"gps_height_binned\"] = \"bin3\"\n",
    "    X.loc[X[\"gps_height\"].between(1567.50,1688.50),\"gps_height_binned\"] = \"bin4\"\n",
    "    X.loc[X[\"gps_height\"].between(1688.50,np.inf),\"gps_height_binned\"] = \"bin5\"\n",
    "    X[\"gps_height_binned\"] = X[\"gps_height_binned\"].astype(\"category\")\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar 0 en amount como dijo optbinning------------------------------------------------------------------------  \n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded\"].copy()\n",
    "    X.loc[X[\"date_recorded\"].between(-np.inf,874.50),\"date_recorded_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"date_recorded\"].between(874.50,981.50),\"date_recorded_binned\"] = \"bin1\"\n",
    "    X.loc[X[\"date_recorded\"].between(981.50,1006.50),\"date_recorded_binned\"] = \"bin2\"\n",
    "    X.loc[X[\"date_recorded\"].between(1006.50,np.inf),\"date_recorded_binned\"] = \"bin3\"\n",
    "    X[\"date_recorded_binned\"] = X[\"date_recorded_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    \n",
    "    #X.loc[X.funder==\"0\",\"funder\"] = np.nan\n",
    "    X[\"funder\"] = X[\"funder\"].replace(\"0\",np.nan)\n",
    "    #X.loc[X.installer==\"0\",\"installer\"] = np.nan\n",
    "    #X.loc[X.installer==\"-\",\"installer\"] = np.nan\n",
    "    X[\"installer\"] = X[\"installer\"].replace(\"0\",np.nan)\n",
    "    X[\"installer\"] = X[\"installer\"].replace(\"-\",np.nan)\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "    drop_list  = [\"management_group\",\"permit\",\"population\",\"amount_tsh\",\"num_private\",\"recorded_by\"]   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #Clusters de latitud y longitud, se hace fuera para que X_test sea transform sin fit------------------------------------------------------------------------\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    \n",
    "    columnas_NA = X.columns[X.isna().any()].tolist()\n",
    "    columnas_NA = [x for x in columnas_NA if x not in ['scheme_name', 'wpt_name', 'ward', 'extraction_type', 'region_code']]# Nas por sobra de categorías, sin nulls en X_test\n",
    "    for i in columnas_NA: \n",
    "        X[i+\"isNA\"] = X[i].copy()\n",
    "        X[i+\"isNA\"]= X[i+\"isNA\"].isna().astype(int)\n",
    "        X[i+\"isNA\"] = X[i+\"isNA\"].astype(\"category\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63c616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X_test = preprocess_train_test(X_test)\n",
    "X = preprocess_train_test(X,X_test)\n",
    "\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "\n",
    "\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan,\n",
    "                                  min_frequency = 0.01), cat_cols),  # si menos de 1% de los datos\n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/QuedandomeConPrimeraParteDeCatsNoEnTest.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ba11e",
   "metadata": {},
   "source": [
    "## PRUEBO Cambios propuestos por validación cruzada.\n",
    "\n",
    "## 0.8156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04726f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X,drop=False,X_test = None):\n",
    "    \"\"\"\n",
    "    pasa X_test sólo al preprocesar X\n",
    "    \"\"\"\n",
    "    if X_test is not None:\n",
    "        cols_mas_unique_en_train = [\"subvillage\",\"installer\",\"funder\",\"scheme_name\",\"wpt_name\",\n",
    "                                    \"region_code\",\"ward\",\"extraction_type\"]\n",
    "        for i in cols_mas_unique_en_train:\n",
    "            unicos_en_test = X_test[i].unique().tolist()\n",
    "            X[i] = [x if x in unicos_en_test else np.nan for x in X[i].values]\n",
    "            #X[i] = [x if x in unicos_en_test else x.split(\" \")[0] for x in X[i].values]\n",
    "\n",
    "    for i in X.select_dtypes(include = \"O\").columns:\n",
    "        X[i] = X[i].astype(\"category\")\n",
    "\n",
    "    #Fechas------------------------------------------------------------------------ \n",
    "    X.date_recorded = X.date_recorded.astype(\"O\")\n",
    "    X.date_recorded = pd.to_datetime(X.date_recorded)\n",
    "    X['date_recorded'] = (X.date_recorded.max() - X.date_recorded).dt.days\n",
    "\n",
    "    #population_binned optbinning------------------------------------------------------------------------ \n",
    "    X[\"population_binned\"] = X[\"population\"].copy()\n",
    "    X.loc[X[\"population\"].between(-np.inf,1.5),\"population_binned\"] = \"bin0\"\n",
    "    X.loc[X[\"population\"].between(1.5,np.inf),\"population_binned\"] = \"bin1\"\n",
    "    X[\"population_binned\"] = X[\"population_binned\"].astype(\"category\")\n",
    "    #Marcar VALORES sin sentido como nan para ser imputados------------------------------------------------------------------------  \n",
    "    X[\"construction_year\"] = X.construction_year.max()-X.construction_year\n",
    "    X.loc[X[\"construction_year\"] ==2013,\"construction_year\"] = np.nan # si da 2013 la resta fue max - cero\n",
    "    #Drop list ------------------------------------------------------------------------ \n",
    "\n",
    "    drop_list  = [\"amount_tsh\",\"num_private\",\"recorded_by\"]   \n",
    "    X.drop(drop_list,axis=1,inplace=True)\n",
    "    #Clusters de latitud y longitud, se hace fuera para que X_test sea transform sin fit------------------------------------------------------------------------\n",
    "    #clustering = KMeans(n_clusters=20)\n",
    "    #X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1ccc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan,\n",
    "                                  min_frequency = 0.005), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/SacoDropsYPreprocesoCambioMinFreq.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467aee41",
   "metadata": {},
   "source": [
    "## Intento otra vez con lo que me dijo cross_validate, pero sin min_freq y con hiperparámetros según gridsearch.\n",
    "\n",
    "## 0.8213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1bbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=False,max_depth=20,max_features=\"sqrt\",min_samples_leaf=2,n_estimators=150,n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/GridSearchRF.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792a842",
   "metadata": {},
   "source": [
    "# Intento otra vez, esta vez con el mejor set de hiperparámetros con bootsrap = True, pues entiendo que esto ayuda al random forest a generalizar mejor.\n",
    "\n",
    "## Empeora el resultado 0.8190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e910e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "#{'bootstrap': True, 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 150}\n",
    "rf = RandomForestClassifier(bootstrap=True,max_depth=20,max_features=\"log2\",min_samples_leaf=1,min_samples_split=3,n_estimators=150,n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/GridSearchBootstrappedRF.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c504f",
   "metadata": {},
   "source": [
    "# Intento otra vez, el mejor hasta ahora, arreglando que no había incluido el paso del preprocesamiento que convertía en Nan las categorías que no aparecen en X_test\n",
    "\n",
    "## Empeoró un poco 0.8208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da575743",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X_test = preprocess(X_test)\n",
    "X = preprocess(X,X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())\n",
    "\n",
    "#{'bootstrap': True, 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 150}\n",
    "rf = RandomForestClassifier(bootstrap=False,max_depth=20,max_features=\"sqrt\",min_samples_leaf=2,n_estimators=150,n_jobs=-1)\n",
    "rf.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)\n",
    "y_pred = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = rf.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "X_test.to_csv(\"Out/ConNANsDeCatsNoEnX_test.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03490c3",
   "metadata": {},
   "source": [
    "# Modelo de voting monstruoso\n",
    "\n",
    "## 0.8151 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee7e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"In/Training_set_values.csv\")\n",
    "y = pd.read_csv(\"In/Training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"In/Test_set_values.csv\")\n",
    "\n",
    "\n",
    "it_imp2 = KNNImputer()\n",
    "\n",
    "for i in X.select_dtypes(include = \"O\").columns:\n",
    "    X[i] = X[i].astype(\"category\")\n",
    "\n",
    "le= LabelEncoder()\n",
    "y[\"status_group\"] = le.fit_transform(y.status_group)\n",
    "\n",
    "\n",
    "X_test = preprocess(X_test)\n",
    "X = preprocess(X,X_test)\n",
    "# no entra en preprocessing para poder hacerle al test solo predict\n",
    "clustering = KMeans(n_clusters=20)\n",
    "X[\"location_cluster\"] = clustering.fit_predict(X[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "X_test[\"location_cluster\"] = clustering.predict(X_test[[\"latitude\",\"longitude\"]].to_numpy())\n",
    "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "num_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('ordinal_enc', OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                  unknown_value=10000000,\n",
    "                                  encoded_missing_value =np.nan), cat_cols),  \n",
    "    ('scaler', StandardScaler(), num_cols),  \n",
    "],\n",
    "remainder = \"passthrough\",\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(data=preprocessing.fit_transform(X,y=y.status_group.values), columns=preprocessing.get_feature_names_out())\n",
    "X_test = pd.DataFrame(data=preprocessing.transform(X_test), columns=preprocessing.get_feature_names_out())\n",
    "\n",
    "\n",
    "#X = it_imp2.fit_transform(X.drop(\"remainder__id\",axis=1))\n",
    "X = pd.DataFrame(data=it_imp2.fit_transform(X), columns=it_imp2.get_feature_names_out())\n",
    "#X_test = it_imp2.transform(X)\n",
    "X_test = pd.DataFrame(data=it_imp2.transform(X_test), columns=it_imp2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d9d22f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;random_fores&#x27;,\n",
       "                              RandomForestClassifier(bootstrap=False,\n",
       "                                                     max_depth=20,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=150)),\n",
       "                             (&#x27;rf_smote&#x27;,\n",
       "                              Pipeline(steps=[(&#x27;smote&#x27;,\n",
       "                                               SMOTE(sampling_strategy=&#x27;minority&#x27;)),\n",
       "                                              (&#x27;rf&#x27;,\n",
       "                                               RandomForestClassifier(bootstrap=False,\n",
       "                                                                      max_depth=20,\n",
       "                                                                      min_samples_leaf=2,\n",
       "                                                                      n_estimators=150))])),\n",
       "                             (&#x27;multilayer_perceptron&#x27;,\n",
       "                              MLPClassifier(max_iter=1000)),\n",
       "                             (&#x27;logistic&#x27;,\n",
       "                              LogisticRegression(max_iter=10000, penalty=&#x27;l1&#x27;,\n",
       "                                                 solver=&#x27;liblinear&#x27;)),\n",
       "                             (&#x27;gradient_boosting&#x27;,\n",
       "                              GradientBoostingClassifier()),\n",
       "                             (&#x27;SupportVectorMachine&#x27;, SVC())],\n",
       "                 weights=[2, 1, 1, 1, 1, 1])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;VotingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;random_fores&#x27;,\n",
       "                              RandomForestClassifier(bootstrap=False,\n",
       "                                                     max_depth=20,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=150)),\n",
       "                             (&#x27;rf_smote&#x27;,\n",
       "                              Pipeline(steps=[(&#x27;smote&#x27;,\n",
       "                                               SMOTE(sampling_strategy=&#x27;minority&#x27;)),\n",
       "                                              (&#x27;rf&#x27;,\n",
       "                                               RandomForestClassifier(bootstrap=False,\n",
       "                                                                      max_depth=20,\n",
       "                                                                      min_samples_leaf=2,\n",
       "                                                                      n_estimators=150))])),\n",
       "                             (&#x27;multilayer_perceptron&#x27;,\n",
       "                              MLPClassifier(max_iter=1000)),\n",
       "                             (&#x27;logistic&#x27;,\n",
       "                              LogisticRegression(max_iter=10000, penalty=&#x27;l1&#x27;,\n",
       "                                                 solver=&#x27;liblinear&#x27;)),\n",
       "                             (&#x27;gradient_boosting&#x27;,\n",
       "                              GradientBoostingClassifier()),\n",
       "                             (&#x27;SupportVectorMachine&#x27;, SVC())],\n",
       "                 weights=[2, 1, 1, 1, 1, 1])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>random_fores</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(bootstrap=False, max_depth=20, min_samples_leaf=2,\n",
       "                       n_estimators=150)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>rf_smote</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">SMOTE</label><div class=\"sk-toggleable__content fitted\"><pre>SMOTE(sampling_strategy=&#x27;minority&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(bootstrap=False, max_depth=20, min_samples_leaf=2,\n",
       "                       n_estimators=150)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>multilayer_perceptron</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(max_iter=1000)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>logistic</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=10000, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>gradient_boosting</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;GradientBoostingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">?<span>Documentation for GradientBoostingClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingClassifier()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>SupportVectorMachine</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('random_fores',\n",
       "                              RandomForestClassifier(bootstrap=False,\n",
       "                                                     max_depth=20,\n",
       "                                                     min_samples_leaf=2,\n",
       "                                                     n_estimators=150)),\n",
       "                             ('rf_smote',\n",
       "                              Pipeline(steps=[('smote',\n",
       "                                               SMOTE(sampling_strategy='minority')),\n",
       "                                              ('rf',\n",
       "                                               RandomForestClassifier(bootstrap=False,\n",
       "                                                                      max_depth=20,\n",
       "                                                                      min_samples_leaf=2,\n",
       "                                                                      n_estimators=150))])),\n",
       "                             ('multilayer_perceptron',\n",
       "                              MLPClassifier(max_iter=1000)),\n",
       "                             ('logistic',\n",
       "                              LogisticRegression(max_iter=10000, penalty='l1',\n",
       "                                                 solver='liblinear')),\n",
       "                             ('gradient_boosting',\n",
       "                              GradientBoostingClassifier()),\n",
       "                             ('SupportVectorMachine', SVC())],\n",
       "                 weights=[2, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=False, max_depth=20, max_features=\"sqrt\", min_samples_leaf=2, n_estimators=150)\n",
    "rf_resampled = Pipeline([\n",
    "    ('smote', SMOTE(sampling_strategy=\"minority\")), \n",
    "    ('rf', RandomForestClassifier(bootstrap=False, max_depth=20, max_features=\"sqrt\", min_samples_leaf=2, n_estimators=150))  \n",
    "])\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)# un multilayer perceptron, entiendo que es como una mini red neuronal\n",
    "log_reg = LogisticRegression(solver = \"liblinear\",max_iter=10000, penalty=\"l1\")# penalty l1 es una regresión lasso, normalmente funcionan mejor en datos con demasiadas columnas\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "SVM = SVC(kernel=\"rbf\")# rbf entiendo es el mejor porque es capaz de encontrar patrones en dimensiones infinitas\n",
    "\n",
    "\n",
    "\n",
    "Clasificador_monstruo = VotingClassifier(estimators=[\n",
    "      ('random_fores', rf),(\"rf_smote\",rf_resampled), ('multilayer_perceptron', mlp),(\"logistic\",log_reg),\n",
    "        ('gradient_boosting', grad_boost),(\"SupportVectorMachine\",SVM)],\n",
    "          weights=[2, 1, 1,1,1,1], voting='hard')\n",
    "\n",
    "Clasificador_monstruo.fit(X.drop(\"remainder__id\",axis=1),y.status_group.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d107d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Clasificador_monstruo.predict(X_test.drop(\"remainder__id\",axis=1))\n",
    "\n",
    "X_test[\"status_group\"] = y_pred\n",
    "\n",
    "X_test.remainder__id = X_test.remainder__id.astype(int)\n",
    "X_test.rename({\"remainder__id\":\"id\",\"predictions\":\"status_group\"},axis=1,inplace=True)\n",
    "\n",
    "X_test = X_test[[\"id\",\"status_group\"]]\n",
    "X_test.status_group = le.inverse_transform(X_test.status_group)\n",
    "\n",
    "X_test.to_csv(\"Out/Clasificador_monstruo.csv\",index=None,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcd14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
